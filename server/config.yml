log:
  level_stream: "INFO"
  level_file: "DEBUG"
  path: "log/logdetective.log"
inference:
  max_tokens: -1
  log_probs: 1
  url: http://llama-cpp-server:8000
  # url: https://mistral-7b-instruct-v0-3--apicast-production.apps.int.stc.ai.prod.us-east-1.aws.paas.redhat.com/
  api_token: ""
  requests_per_minute: 6
  http_timeout: 5.0
  http_connect_timeout: 3.07
# Separate LLM endpoint for snippet analysis, optional
# snippet_inference:
#   max_tokens: -1
#   log_probs: 1
#   url: http://llama-cpp-server:8000
#   api_token: ""
#   requests_per_minute: 6
extractor:
  context: true
  max_clusters: 25
  verbose: false
gitlab:
  "GitLab SaaS":
    url: https://gitlab.com
    api_token: glpat-XXXXXX
    webhook_secrets: []
    max_artifact_size: 300
  "GitLab Internal":
    url: https://gitlab.example.com
    api_token: glpat-XXXXXX
    max_artifact_size: 300
    webhook_secrets:
      - example_secret
general:
  devmode: False
  packages:
    - .*
  excluded_packages:
    - ^redhat-internal-.*
