log:
  level_stream: "INFO"
  level_file: "DEBUG"
  path: "log/logdetective.log"
inference:
  max_tokens: 10000 # maximum number of tokens must be a positive integer
  log_probs: true
  url: http://llama-cpp-server:8000
  # url: https://mistral-7b-instruct-v0-3--apicast-production.apps.int.stc.ai.prod.us-east-1.aws.paas.redhat.com/
  api_token: "XXX" # Must not be empty, even if the server doesn't require authentication
  requests_per_minute: 6
  http_timeout: 5.0
  http_connect_timeout: 3.07
  # Roles are used to format prompts into chat messages
  # If the roles are same, the system prompt and user message are concatenated
  # user_role: user
  # system_role: developer
# Separate LLM endpoint for snippet analysis, optional
# snippet_inference:
#   max_tokens: -1
#   log_probs: true
#   url: http://llama-cpp-server:8000
#   api_token: ""
#   requests_per_minute: 6
extractor:
  context: true
  max_clusters: 25
  verbose: false
gitlab:
  "GitLab SaaS":
    url: https://gitlab.com
    api_token: glpat-XXXXXX
    webhook_secrets: []
    max_artifact_size: 300
    timeout: 5.0
  "GitLab Internal":
    url: https://gitlab.example.com
    api_token: glpat-XXXXXX
    max_artifact_size: 300
    webhook_secrets:
      - example_secret
    timeout: 6.0
koji:
  "fedora":
    xmlrpc_url: https://koji.fedoraproject.org/kojihub
    max_artifact_size: 300
    tokens:
      - example_token
general:
  devmode: False
  packages:
    - .*
  excluded_packages:
    - ^redhat-internal-.*
