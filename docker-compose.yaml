services:
  llama-cpp:
    image: quay.io/logdetective/inference:latest
    build:
      context: .
      dockerfile: ./Containerfile.llama.cpp
    # make sure to match this hostname with inference->url in server/config.yml
    hostname: llama-cpp-server
    command: "llama-server --host 0.0.0.0 --port ${LLAMA_CPP_SERVER_PORT:-8000}"
    stdin_open: true
    tty: true
    env_file: .env
    ports:
      - "${LLAMA_CPP_SERVER_PORT:-8000}:${LLAMA_CPP_SERVER_PORT:-8000}"
    volumes:
      - ${MODELS_PATH-./models}:/models:Z
    # these 4 lines are needed for CUDA acceleration
    # devices:
    #   - nvidia.com/gpu=all
    # security_opt:
    #   - "label=disable"
  server:
    image: quay.io/logdetective/server:latest
    depends_on:
      - postgres
    build:
      context: .
      dockerfile: ./Containerfile
    hostname: logdetective-server
    stdin_open: true
    tty: true
    volumes:
      - .:/src/:z
      - ./server/config.yml:/config.yml:z
      - ./files/run_server.sh:/run_server.sh:z
      - matplotlib-config:${MPLCONFIGDIR}
    ports:
      - "${LOGDETECTIVE_SERVER_PORT:-8080}:${LOGDETECTIVE_SERVER_PORT:-8080}"
    env_file: .env
    # so gunicorn can find logdetective python module
    # and alembic can find alembic.ini
    working_dir: /src
    entrypoint: ["/run_server.sh"]

  postgres:
    image: quay.io/sclorg/postgresql-15-c9s
    volumes:
      - database_data:/var/lib/postgresql/data
    ports:
      - "${POSTGRESQL_PORT:-5432}:${POSTGRESQL_PORT:-5432}"
    env_file: .env

  packit-interface:
    image: quay.io/logdetective/packit:latest
    depends_on:
      - server
    # Port 8090 is the default for logdetective-packit container
    # github.com/fedora-copr/logdetective-packit
    ports:
      - "${PACKIT_INTERFACE_PORT:-8090}:${PACKIT_INTERFACE_PORT:-8090}"
    env_file: .env

volumes:
  database_data:
    driver: local
  matplotlib-config:
    driver: local
