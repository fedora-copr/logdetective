version: "3"
services:
  llama-cpp:
    build:
      context: .
    hostname: "${LLAMA_CPP_HOST}"
    command: "python3 -m llama_cpp.server --model ${MODEL_FILEPATH} --host 0.0.0.0 --port ${LLAMA_CPP_SERVER_PORT}"
    stdin_open: true
    tty: true
    env_file: .env
    ports:
      - "${LLAMA_CPP_SERVER_PORT-8090}:${LLAMA_CPP_SERVER_PORT-8090}"
    volumes:
      - ${MODELS_PATH-./models}:/models:Z
  server:
    build:
      context: .
    hostname: logdetective-server
    stdin_open: true
    tty: true
    volumes:
      - .:/src/:z
    ports:
      - ${LOGDETECTIVE_SERVER_PORT-8080}:${LOGDETECTIVE_SERVER_PORT-8080}
    env_file: .env
    # --no-reload: doesn't work in a container - `PermissionError: Permission denied (os error 13) about ["/proc"]`
    command: fastapi dev /src/logdetective/server.py --host 0.0.0.0 --port $LOGDETECTIVE_SERVER_PORT --no-reload
