services:
  llama-cpp:
    extends:
      file: docker-compose.yaml
      service: llama-cpp
    # these lines are needed for CUDA acceleration
    devices:
      - nvidia.com/gpu=all
    environment:
      LLAMA_ARG_N_PARALLEL: 16
  server:
    extends:
      file: docker-compose.yaml
      service: server
    environment:
      ENV: production
      LOGDETECTIVE_SERVER_PORT: 443
  postgres:
    extends:
      file: docker-compose.yaml
      service: postgres

volumes:
  database_data:
    driver: local
  matplotlib-config:
    driver: local
