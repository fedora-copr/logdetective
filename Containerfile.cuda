FROM fedora:39
# make sure nvidia driver match on host and in the container
RUN dnf install -y python3-requests python3-pip gcc gcc-c++ python3-scikit-build git-core \
    && echo "[cuda-fedora39-x86_64]" >> /etc/yum.repos.d/cuda.repo \
    && echo "name=cuda-fedora39-x86_64" >> /etc/yum.repos.d/cuda.repo \
    && echo "baseurl=https://developer.download.nvidia.com/compute/cuda/repos/fedora39/x86_64" >> /etc/yum.repos.d/cuda.repo \
    && echo "enabled=1" >> /etc/yum.repos.d/cuda.repo \
    && echo "gpgcheck=1" >> /etc/yum.repos.d/cuda.repo \
    && echo "gpgkey=https://developer.download.nvidia.com/compute/cuda/repos/fedora39/x86_64/D42D0685.pub" >> /etc/yum.repos.d/cuda.repo \
    && dnf module enable -y nvidia-driver:555-dkms \
    && dnf install -y cuda-compiler-12-6 cuda-toolkit-12-6 nvidia-driver-cuda-libs cmake \
    && dnf clean all
ENV LLAMACPP_VER="b4501"
ENV PATH=${PATH}:/usr/local/cuda-12.6/bin/

# Clone, checkout, build and move llama.cpp server to path
RUN git clone https://github.com/ggerganov/llama.cpp.git && \
    cd llama.cpp && \
    git checkout $LLAMACPP_VER && \
    cmake -B build -DGGML_CUDA=ON && \
    cmake --build build --config Release -j 4 -t llama-server && \
    mv ./build/bin/llama-server /usr/bin/llama-server
